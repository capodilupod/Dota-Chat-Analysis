{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import json\n",
    "\n",
    "import string as s\n",
    "from collections import OrderedDict\n",
    "from random import randint\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"match_chat_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Match Id</th>\n",
       "      <th>Match Duration</th>\n",
       "      <th>Radiant Chat</th>\n",
       "      <th>Dire Chat</th>\n",
       "      <th>Radiant Win</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3548935602</td>\n",
       "      <td>2517</td>\n",
       "      <td>[('GOTTEM', 1500), ('LOL', 1578), ('The triple...</td>\n",
       "      <td>[('GOTTEM', 1570), ('NIOCE MEDUSA', 2525)]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3548935303</td>\n",
       "      <td>2644</td>\n",
       "      <td>[('peru', 8), ('peru', 33), ('peru', 1587), ('...</td>\n",
       "      <td>[('MATENLO AL PUDGE ESTA SOLO ARRIBA :V', -14)...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3548935205</td>\n",
       "      <td>3629</td>\n",
       "      <td>[('is he coming back ?', 228), ('hes not comin...</td>\n",
       "      <td>[('+', 228), ('СУКА', 369), ('no', 377), ('thx...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3548935007</td>\n",
       "      <td>2514</td>\n",
       "      <td>[('leave our jungle you monkey', -24), (\"i'm a...</td>\n",
       "      <td>[('xD', -20), ('shut up tiger', -12), ('hahahh...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3548934300</td>\n",
       "      <td>2339</td>\n",
       "      <td>[]</td>\n",
       "      <td>[('aw', 383), ('coming', 525), ('babi', 680), ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0    Match Id  Match Duration  \\\n",
       "0           0  3548935602            2517   \n",
       "1           1  3548935303            2644   \n",
       "2           2  3548935205            3629   \n",
       "3           3  3548935007            2514   \n",
       "4           4  3548934300            2339   \n",
       "\n",
       "                                        Radiant Chat  \\\n",
       "0  [('GOTTEM', 1500), ('LOL', 1578), ('The triple...   \n",
       "1  [('peru', 8), ('peru', 33), ('peru', 1587), ('...   \n",
       "2  [('is he coming back ?', 228), ('hes not comin...   \n",
       "3  [('leave our jungle you monkey', -24), (\"i'm a...   \n",
       "4                                                 []   \n",
       "\n",
       "                                           Dire Chat  Radiant Win  \n",
       "0         [('GOTTEM', 1570), ('NIOCE MEDUSA', 2525)]         True  \n",
       "1  [('MATENLO AL PUDGE ESTA SOLO ARRIBA :V', -14)...         True  \n",
       "2  [('+', 228), ('СУКА', 369), ('no', 377), ('thx...        False  \n",
       "3  [('xD', -20), ('shut up tiger', -12), ('hahahh...         True  \n",
       "4  [('aw', 383), ('coming', 525), ('babi', 680), ...        False  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create the dictionary of possible words to create our feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_dict(df):\n",
    "    \"\"\"Get a dictionary of unique words where key is the word and value is the order it was added.\"\"\"\n",
    "    word_dict = {}\n",
    "    count = 0\n",
    "    \n",
    "    for row in df.itertuples():\n",
    "        #get radiant words\n",
    "        radiant_chat = row[4]\n",
    "        dire_chat = row[5]\n",
    "        \n",
    "        #remove brackets and parantheses, and single quotes\n",
    "        radiant_chat = radiant_chat.replace(\"(\", \"\").replace(\")\", \"\").replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\")\n",
    "        dire_chat = dire_chat.replace(\"(\", \"\").replace(\")\", \"\").replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\")\n",
    "        \n",
    "        all_chat = radiant_chat + \", \" + dire_chat\n",
    "        \n",
    "        #remove punctuation and replace with spaces\n",
    "        for i in s.punctuation:\n",
    "            all_chat = all_chat.replace(i, \" \")\n",
    "        all_chat = all_chat.lower()\n",
    "        for word in all_chat.split(\" \"):\n",
    "            if not word.isdigit() and word not in word_dict:\n",
    "                word_dict[word] = count\n",
    "                count += 1\n",
    "    \n",
    "    return word_dict\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10716"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dict = get_word_dict(df)\n",
    "len(word_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we form our feature vectors for each teams chat so there will be number of games x 2 feature vectors as we will ahve a feature vector for each team in a game (dire and radiant). Feature vectors will be of the form "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_feature_matrix(df, word_dict, portion_of_match=1):\n",
    "    \"\"\"Takes in the dataframe of match information (chat and outcome) and \n",
    "    creates feature matrix of chat feature vectors.\n",
    "    df - dataframe of match data \n",
    "    word_dict - dictionary of all possible words used\n",
    "    portion_of_match - number between (0,1) indicating which part of the match you want to take words from\n",
    "                        with 1 being all the match and 0 being none of it.\n",
    "    \"\"\"\n",
    "   \n",
    "    feature_matrix = []\n",
    "    labels = [] # 0 indicates a loss 1 is a win\n",
    "    \n",
    "    for row in df.itertuples():\n",
    "        #get radiant and dire chats\n",
    "        radiant_chat = row[4]\n",
    "        dire_chat = row[5]\n",
    "        radiant_win = row[6]\n",
    "        match_duration = int(row[3])\n",
    "        match_portion_max_time = portion_of_match * match_duration\n",
    "        \n",
    "        #for each feature vector, we append them in the order radiant then dire\n",
    "        if radiant_win == True:\n",
    "            labels.append(1)\n",
    "            labels.append(0)\n",
    "        else:\n",
    "            labels.append(0)\n",
    "            labels.append(1)\n",
    "        \n",
    "        #remove brackets and parantheses, and single quotes\n",
    "        radiant_chat = radiant_chat.replace(\"(\", \"\").replace(\")\", \"\").replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\")\n",
    "        dire_chat = dire_chat.replace(\"(\", \"\").replace(\")\", \"\").replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\")\n",
    "        \n",
    "        #remove punctuation and replace with spaces\n",
    "        for i in s.punctuation:\n",
    "            radiant_chat = radiant_chat.replace(i, \" \")\n",
    "            dire_chat = dire_chat.replace(i, \" \")\n",
    "        \n",
    "        radiant_chat = radiant_chat.lower()\n",
    "        dire_chat = dire_chat.lower()\n",
    "        \n",
    "        radiant_vector = np.zeros(len(word_dict))\n",
    "        dire_vector = np.zeros(len(word_dict))\n",
    "        \n",
    "        skip_next = False\n",
    "        for word in radiant_chat.split():\n",
    "            if not word.isdigit() and skip_next == False:\n",
    "                radiant_vector[word_dict[word]] = 1\n",
    "            if word.isdigit() and int(word) <= match_portion_max_time:\n",
    "                skip_next = False\n",
    "            else:\n",
    "                skip_next = True\n",
    "                \n",
    "        skip_next = False \n",
    "        for word in dire_chat.split():\n",
    "            if not word.isdigit():\n",
    "                dire_vector[word_dict[word]] = 1\n",
    "            if word.isdigit() and int(word) <= match_portion_max_time:\n",
    "                skip_next = False\n",
    "            else:\n",
    "                skip_next = True\n",
    "        \n",
    "        feature_matrix.append(radiant_vector)\n",
    "        feature_matrix.append(dire_vector)\n",
    "        \n",
    "    feature_matrix = np.asmatrix(feature_matrix)\n",
    "    labels = np.asarray(labels)\n",
    "    return feature_matrix, labels\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix, labels = generate_feature_matrix(df, word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_performance(clf, X, y, k=5):\n",
    "    \"\"\"\n",
    "    Splits the data, X and y, into k-folds and runs k-fold crossvalidation:\n",
    "    training a classifier on K-1 folds and testing on the remaining fold.\n",
    "    Calculates the k-fold crossvalidation performance metric for classifier\n",
    "    clf by averaging the performance across folds.\n",
    "    Input:\n",
    "    clf- an instance of SVC()\n",
    "    X- (n,d) array of feature vectors, where n is the number of examples\n",
    "       and d is the number of features\n",
    "    y- (n,) array of binary labels {1,-1}\n",
    "    k- int specificyin the number of folds (default=5)\n",
    "    Returns: average 'test' performance across the k folds as np.float64\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits = k);\n",
    "    iteration = 0;\n",
    "    average = 0;\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        clf.fit(X_train, y_train);\n",
    "        \n",
    "        print(\"for iteration \" + str(iteration));\n",
    "        accuracy = metrics.accuracy_score(y_test, clf.predict(X_test));\n",
    "        average = average + accuracy;\n",
    "        print(str(accuracy));\n",
    "        iteration = iteration + 1;\n",
    "    \n",
    "    average = average / k;\n",
    "    return average;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel = 'linear', class_weight = 'balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for iteration 0\n",
      "0.625\n",
      "for iteration 1\n",
      "0.615\n",
      "for iteration 2\n",
      "0.5975\n",
      "for iteration 3\n",
      "0.6325\n",
      "for iteration 4\n",
      "0.64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.622"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_performance(clf, feature_matrix, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next try the same approach but this time training on a neural net with 1-layer and 100 neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_matrix, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# Fit only to the training data\n",
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now apply the transformations to the data:\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=100, learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[115 137]\n",
      " [ 84 164]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix, accuracy_score\n",
    "print(confusion_matrix(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.58      0.46      0.51       252\n",
      "          1       0.54      0.66      0.60       248\n",
      "\n",
      "avg / total       0.56      0.56      0.55       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.558\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural net with 1 hidden layer and 100 neurons gives us only a 56% accuracy. Lets try adding 2 more hidden layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(100,100,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100, 100, 100), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.552\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the number of hidden layers did not do much to accuracy. Lets play with the number of neurons now in each layer and see what effect that has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.588\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(500,500,500))\n",
    "mlp.fit(X_train,y_train)\n",
    "predictions = mlp.predict(X_test)\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the number of neurons decreased our accuracy by 4 percent. Lets go back to 100 neurons per layer and add in another layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.584\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(100,100,100,100))\n",
    "mlp.fit(X_train,y_train)\n",
    "predictions = mlp.predict(X_test)\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.586\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(100,100,100))\n",
    "mlp.fit(X_train,y_train)\n",
    "predictions = mlp.predict(X_test)\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Match Prediction at Different Points of The Match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use our model to predict the match outcome based on chat from using only a quarter of the match, half the match, then three quarters of the match. We hypothesize that the accuracy will increase monotonically as losing players will express their frustrations more in chat as the game swings more in the winning teams favor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix_quarter_match, labels = generate_feature_matrix(df, word_dict, .25)\n",
    "feature_matrix_half_match, labels = generate_feature_matrix(df, word_dict, .5)\n",
    "feature_matrix_three_quarter_match, labels = generate_feature_matrix(df, word_dict, .75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for iteration 0\n",
      "0.545\n",
      "for iteration 1\n",
      "0.5675\n",
      "for iteration 2\n",
      "0.5625\n",
      "for iteration 3\n",
      "0.5675\n",
      "for iteration 4\n",
      "0.56\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5605"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_performance(clf, feature_matrix_quarter_match, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for iteration 0\n",
      "0.595\n",
      "for iteration 1\n",
      "0.5775\n",
      "for iteration 2\n",
      "0.575\n",
      "for iteration 3\n",
      "0.5975\n",
      "for iteration 4\n",
      "0.555\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.57999999999999996"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_performance(clf, feature_matrix_half_match, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for iteration 0\n",
      "0.61\n",
      "for iteration 1\n",
      "0.58\n",
      "for iteration 2\n",
      "0.5625\n",
      "for iteration 3\n",
      "0.6125\n",
      "for iteration 4\n",
      "0.585\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.59000000000000008"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_performance(clf, feature_matrix_three_quarter_match, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
